{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DBSCAN as clustering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"INITIAL\",\"login\",\"View_Items\",\"home\",\"logout\",\"View_Items_quantity\",\"Add_to_Cart\",\"shoppingcart\",\n",
    "          \"remove\",\"deferorder\",\"purchasecart\",\"inventory\",\"sellinventory\",\"clearcart\",\"cancelorder\",\"$\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = {'HZKS0-WG8pZr0eCsZlBAP5Xm': ['login',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'home',\n",
    "  'logout'],\n",
    " '5tPgZbHdK2Zp+heFBs8HsMkx': ['login',\n",
    "  'View_Items_quantity',\n",
    "  'Add_to_Cart',\n",
    "  'View_Items_quantity',\n",
    "  'Add_to_Cart',\n",
    "  'View_Items_quantity',\n",
    "  'Add_to_Cart',\n",
    "  'View_Items_quantity',\n",
    "  'Add_to_Cart',\n",
    "  'View_Items_quantity',\n",
    "  'Add_to_Cart',\n",
    "  'shoppingcart',\n",
    "  'remove',\n",
    "  'shoppingcart',\n",
    "  'remove',\n",
    "  'shoppingcart',\n",
    "  'remove',\n",
    "  'shoppingcart',\n",
    "  'remove',\n",
    "  'deferorder',\n",
    "  'home',\n",
    "  'logout'],\n",
    " 'RU2oCVNdpWEM0-2x7I5OjPbZ': ['login',\n",
    "  'View_Items_quantity',\n",
    "  'Add_to_Cart',\n",
    "  'View_Items_quantity',\n",
    "  'Add_to_Cart',\n",
    "  'purchasecart',\n",
    "  'home',\n",
    "  'logout'],\n",
    " 'kG4g0E5mqwRYcsQOCfj+7wG7': ['login',\n",
    "  'inventory',\n",
    "  'inventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'home',\n",
    "  'logout'],\n",
    " '8ocO6WP3QaFpBvkooS5INPwe': ['login',\n",
    "  'inventory',\n",
    "  'inventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'home',\n",
    "  'logout'],\n",
    " 'WOTZQBwSCnI+DfDQ-2cS7Mgp': ['login',\n",
    "  'inventory',\n",
    "  'inventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'home',\n",
    "  'logout'],\n",
    " 'e4bMe1HfiUlmvUMmPJU4y1B4': ['login',\n",
    "  'inventory',\n",
    "  'inventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'sellinventory',\n",
    "  'home',\n",
    "  'logout'],\n",
    " 'MEzDpcnm1MQ9GFGox7uP4Ep-': ['login',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'View_Items',\n",
    "  'home',\n",
    "  'logout']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Makovchain & sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transition_matrix(sessions, states):\n",
    "    markovchains = []\n",
    "    for key, value in sessions.items():\n",
    "        # labelEncoding\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(value)\n",
    "        transformed_s = le.transform(value)\n",
    "\n",
    "        #factorize\n",
    "        factorize = pd.factorize(value)[0]\n",
    "        \n",
    "        # matrix\n",
    "        n = 1 + max(factorize)  # number of states\n",
    "        M = [[0] * n for _ in range(n)]\n",
    "\n",
    "        for (i, j) in zip(factorize, factorize[1:]):\n",
    "            M[i][j] += 1\n",
    "        \n",
    "        # now convert to probabilities:\n",
    "        for row in M:\n",
    "            s = sum(row)\n",
    "            if s > 0:\n",
    "                row[:] = [f / s for f in row]\n",
    "                \n",
    "        # print Matrix style\n",
    "        #for row in M: print(' '.join('{0:.2f}'.format(x) for x in row))\n",
    "        \n",
    "        # unique array in the right order\n",
    "        value = np.array(value)\n",
    "        _, idx = np.unique(value, return_index=True)\n",
    "        \n",
    "        df = pd.DataFrame(data = M, index=value[np.sort(idx)],\n",
    "                          columns=value[np.sort(idx)])\n",
    "\n",
    "        df_1 = pd.DataFrame(index=states, columns=states, dtype='float64')\n",
    "\n",
    "        #merge = df_1.merge(df, how='right').fillna(0).round(2).set_index(value[np.sort(idx)])\n",
    "        merge = df_1.update(df, join='left')\n",
    "        #merge = pd.merge(df_1, df, how='right').fillna(0).round(2).set_index(value[np.sort(idx)])\n",
    "        \n",
    "        \n",
    "        merge = pd.concat([pd.concat([df_1, df], axis=1, sort= False)], axis=0).fillna(0).round(2).iloc[:, :-n] \n",
    "        #merge = merge.iloc[:, :-n]        \n",
    "        \n",
    "        \n",
    "        # convert into Vector\n",
    "        merge = np.array(merge.values.flatten().tolist())\n",
    "        #print(len(merge))\n",
    "        # resize so the vectors got the same length\n",
    "        #size = 16*16\n",
    "        #merge.resize(size)\n",
    "        \n",
    "        # 2-D array \n",
    "        markovchains.append(merge)\n",
    "        #print(len(markovchains))\n",
    "        # csr sparse matrix\n",
    "        csr = csr_matrix(markovchains)\n",
    "        #print(csr.shape)\n",
    "        #markovchains.append(merge)\n",
    "        \n",
    "    #print(len(merge))\n",
    "    return csr\n",
    "\n",
    "#m = transition_matrix(sessions, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 -1 -1  0  0  0  0  0]\n",
      "(array([-1,  0], dtype=int64), array([2, 6], dtype=int64))\n",
      "DBSCAN(algorithm='auto', eps=2, leaf_size=30, metric='euclidean',\n",
      "    metric_params=None, min_samples=2, n_jobs=None, p=None)\n"
     ]
    }
   ],
   "source": [
    "X = m\n",
    "clustering = DBSCAN(eps=2, min_samples=2).fit(X)\n",
    "labels = clustering.labels_\n",
    "print(labels)\n",
    "print(np.unique(labels, return_counts=True))\n",
    "print(clustering)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test\n",
    "X = np.arange(100).reshape(20,5)\n",
    "clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n",
    "labels = clustering.labels_M\n",
    "print(labels)\n",
    "print(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagate after clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data imports\n",
    "PATH = \"../../data/raw/\"\n",
    "sessions_file = (PATH+'sessions.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_request_dict(sessions_file):\n",
    "    s_r_dict = {}\n",
    "    # Dict of sessions\n",
    "    with open(sessions_file) as fn:\n",
    "        sessions_raw = fn.readlines()\n",
    "\n",
    "    for session in sessions_raw:\n",
    "        key = re.search('([^.]+)', session).group()\n",
    "        value = re.findall('\\\"(.*?)\\\"', session)\n",
    "        s_r_dict[key] = value\n",
    "\n",
    "    return s_r_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = session_request_dict(sessions_file)\n",
    "\n",
    "set_1 = {k: data[k] for k in list(data)[0:1000]}\n",
    "set_2 = {k: data[k] for k in list(data)[500:1500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dict_Cluster\n",
    "def cluster_dict(labels, X_):\n",
    "    cluster_list =[]\n",
    "    \n",
    "    for label in np.unique(labels):\n",
    "        points = X_[labels == label].toarray()\n",
    "        \n",
    "        for point in points:\n",
    "            cluster_dict = {}\n",
    "            cluster_dict[label] = point\n",
    "            cluster_list.append(cluster_dict)\n",
    "            \n",
    "    return cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2], dtype=int64), array([485, 261, 254], dtype=int64))\n",
      "(array([0, 1, 2], dtype=int64), array([283, 481, 236], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "X_1 = transition_matrix(set_1, states)\n",
    "X_2 = transition_matrix(set_2, states)\n",
    "\n",
    "#print('matrix done', datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "#print('start clustering\\n')\n",
    "\n",
    "clustering_1 = DBSCAN(eps=1.5, min_samples=10).fit(X_1)\n",
    "clustering_2 = DBSCAN(eps=1.5, min_samples=10).fit(X_2)\n",
    "\n",
    "labels_1 = clustering_1.labels_\n",
    "labels_2 = clustering_2.labels_\n",
    "\n",
    "# clustering1.components_.toarray()\n",
    "#n_clusters = len(np.unique(labels))\n",
    "#first_list = X_1[labels_1 == 0].toarray()\n",
    "#second_list = X_2[labels_2 == 0].toarray()\n",
    "\n",
    "\n",
    "cluster_dict_1 = cluster_dict(labels_1, X_1)\n",
    "cluster_dict_2 = cluster_dict(labels_2, X_2)\n",
    "\n",
    "print(np.unique(labels_1, return_counts=True))\n",
    "print(np.unique(labels_2, return_counts=True))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(np.unique(labels_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- array mit den labels rausziehen, dass dann so dynamsich zu gestalten for list_cluster, list cluster itteriert durch und erstellt die listen (first list, usw.) diese sollten dann weitergeleitet werden in eine funktion die dynmisch die brechnungne macht. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01171875], [0.022493414750957855], [0.015748031496062992]]\n"
     ]
    }
   ],
   "source": [
    "def list_cluster(cluster_dict_):\n",
    "    cluster_list = []\n",
    "    if labels_1 in labels_2:\n",
    "        for cluster_index, value in enumerate(np.unique(labels_1)):\n",
    "            tmp = []\n",
    "            for item in cluster_dict_:\n",
    "                for k,v in item.items():\n",
    "                    if k == cluster_index:\n",
    "                        tmp.append(v.tolist())\n",
    "            cluster_list.append([np.mean(tmp)])\n",
    "    return cluster_list\n",
    "\n",
    "first_list = list_cluster(cluster_dict_1)\n",
    "\n",
    "second_list = list_cluster(cluster_dict_2)\n",
    "print(first_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01577683303886926], [0.01171875], [0.022608746027542374]]\n"
     ]
    }
   ],
   "source": [
    "print(second_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def list_cluster(cluster_dict_):\n",
    "    c0 = []\n",
    "    for item in cluster_dict_:\n",
    "        for k,v in item.items():\n",
    "            if k == 0:\n",
    "                c0.append(v.tolist())\n",
    "    return c0\n",
    "first_list_0 = list_cluster(cluster_dict_1)\n",
    "\n",
    "second_list_0 = list_cluster(cluster_dict_2)\n",
    "\n",
    "\n",
    "def list_cluster(cluster_dict_):\n",
    "    c0 = []\n",
    "    for item in cluster_dict_:\n",
    "        for k,v in item.items():\n",
    "            if k == 1:\n",
    "                c0.append(v.tolist())\n",
    "    return c0\n",
    "first_list_1 = list_cluster(cluster_dict_1)\n",
    "\n",
    "second_list_1 = list_cluster(cluster_dict_2)\n",
    "\n",
    "\n",
    "def list_cluster(cluster_dict_):\n",
    "    c0 = []\n",
    "    for item in cluster_dict_:\n",
    "        for k,v in item.items():\n",
    "            if k == 2:\n",
    "                c0.append(v.tolist())\n",
    "    return c0\n",
    "first_list_2 = list_cluster(cluster_dict_1)\n",
    "\n",
    "second_list_2 = list_cluster(cluster_dict_2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "differences = []\n",
    "\n",
    "for list in first_list_0:\n",
    "    if list not in second_list_0:\n",
    "        differences.append(list)\n",
    "print(len(differences))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "differences = []\n",
    "\n",
    "for list in first_list_1:\n",
    "    if list not in second_list_1:\n",
    "        differences.append(list)\n",
    "print(len(differences))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "differences = []\n",
    "\n",
    "for list in first_list_2:\n",
    "    if list not in second_list_2:\n",
    "        differences.append(list)\n",
    "print(len(differences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kreuztest Verschiebung der Cluster"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "differences = []\n",
    "\n",
    "for list in first_list_1:\n",
    "    if list not in second_list_2:\n",
    "        differences.append(list)\n",
    "print(len(differences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "    \n",
    "https://stackoverflow.com/questions/18237479/dbscan-in-scikit-learn-of-python-save-the-cluster-points-in-an-array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'subtract' did not contain a loop with signature matching types dtype('<U32') dtype('<U32') dtype('<U32')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b21cbce078f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msecond_list_tmp_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msecond_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecond_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mmin_point_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecond_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mvalue_subtraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecond_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'subtract' did not contain a loop with signature matching types dtype('<U32') dtype('<U32') dtype('<U32')"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: Durchlauf, check if liste != dann die \"alte\" liste benutzen. Dann müssen diese punkte gar nicht mehr neu genommen werden\n",
    "\"\"\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "mylist = [first_list, second_list]\n",
    "old_min_points = {}\n",
    "cluster_mean_history={}\n",
    "\n",
    "p\n",
    "for index, value in enumerate(first_list):\n",
    "    \n",
    "    min_point_dict = {}\n",
    "    value_subtraction = []\n",
    "    second_list_tmp_dict = {}\n",
    "    \n",
    "    for second_index, second_value in enumerate(second_list):\n",
    "        \n",
    "        second_list_tmp_dict[second_index]=second_value\n",
    "        \n",
    "        min_point_dict[abs(np.array(value)-np.array(second_value))[0]]=[index, second_index]\n",
    "        \n",
    "        value_subtraction.append(abs(np.array(value)-np.array(second_value)))\n",
    "        \n",
    "\n",
    "    old_min_points[min_point_dict[min(value_subtraction)[0]][0]] = second_list_tmp_dict[\n",
    "        min_point_dict[min(value_subtraction)[0]][1]\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"cluster_mean_history[min_point_dict[min(value_subtraction)[0]][0]] = second_list_tmp_dict[\n",
    "    min_point_dict[min(value_subtraction)[0]][1]\n",
    "    ]\n",
    "    \n",
    "    t = expand(cluster_mean_history, index)\n",
    "    print(t)\"\"\"\n",
    "    \n",
    "    \n",
    "        \n",
    "    #print(\"min-point\",min(value_subtraction)) #index noch außerhalb\n",
    "    #print(min_point_dict[min(value_subtraction)[0]][1])\n",
    "\n",
    "#old_min_points\n",
    "cluster_mean_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(cluster_mean_history, index):\n",
    "    list = cluster_mean_history[index]\n",
    "    list.append(\"123\")\n",
    "    return list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    cluster_mean_history[min_point_dict[min(value_subtraction)[0]][0]] = [second_list_tmp_dict[\n",
    "        min_point_dict[min(value_subtraction)[0]][1], value\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "first_list = [[1, 3], [13, 2]]\n",
    "first_list_1 = [[1, 3], [13, 2]]\n",
    "second_list = [[1, 2], [13, 2]]\n",
    "print(len(np.array(first_list).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 1}, {1: 1}, {2: 2}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "differences = []\n",
    "\n",
    "mylist = [first_list,first_list_1,second_list]\n",
    "\n",
    "for index, value in enumerate(mylist):\n",
    "    tmp = []\n",
    "    for elements in mylist[:]:\n",
    "        for list in value:\n",
    "            if list not in elements:\n",
    "                differences_dict = {}\n",
    "                tmp.append(list)\n",
    "                differences_dict[index]=len(tmp)\n",
    "                if len(tmp) > 1:\n",
    "                    del differences[-1]\n",
    "                differences.append(differences_dict)\n",
    "differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 3], [13, 2]], [[1, 3], [13, 2]], [[1, 2], [13, 2]]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Punkte in einem Cluster miteinander vergleichen.\n",
    "\n",
    "https://stackoverflow.com/questions/16603282/how-to-compare-each-item-in-a-list-with-the-rest-only-once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tuple_list = [tuple(lst) for lst in first_list]\n",
    "second_tuple_list = [tuple(lst) for lst in second_list]\n",
    "#print(first_tuple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_set = set(first_tuple_list)\n",
    "second_set = set(second_tuple_list)\n",
    "#print(first_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_set = set(map(tuple, first_list))\n",
    "second_set = set(map(tuple, second_list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set(first_set) & set(second_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frozenset(first_set).intersection(second_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: \n",
    "\n",
    "https://stackoverflow.com/questions/6105777/how-to-compare-a-list-of-lists-sets-in-python\n",
    "https://stackoverflow.com/questions/1388818/how-can-i-compare-two-lists-in-python-and-return-matches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
